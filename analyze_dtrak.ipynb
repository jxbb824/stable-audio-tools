{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D-TRAK Attribution Analysis\n",
    "\n",
    "Analyze training data attribution for generated audio samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "from IPython.display import Audio, display, HTML\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Attribution Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "output_dir = Path(\"/home/xiruij/stable-audio-tools/outputs/dtrak_attribution_20260216_005732\")\n",
    "\n",
    "# Load metadata\n",
    "with open(output_dir / \"scores_query_x_train.memmap.meta.json\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "print(f\"Query samples: {meta['query_num_examples']}\")\n",
    "print(f\"Train samples: {meta['train_num_examples']}\")\n",
    "print(f\"Score shape: {meta['shape']}\")\n",
    "\n",
    "# Load scores (query x train)\n",
    "scores = np.memmap(\n",
    "    output_dir / \"scores_query_x_train.memmap\",\n",
    "    dtype=np.float32,\n",
    "    mode=\"r\",\n",
    "    shape=tuple(meta['shape'])\n",
    ")\n",
    "\n",
    "# Load IDs\n",
    "with open(output_dir / \"query_features.memmap.ids.txt\") as f:\n",
    "    query_ids = [line.strip() for line in f]\n",
    "\n",
    "with open(output_dir / \"train_features.memmap.ids.txt\") as f:\n",
    "    train_ids = [line.strip() for line in f]\n",
    "\n",
    "print(f\"\\nLoaded {len(query_ids)} query IDs and {len(train_ids)} train IDs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Attribution Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot score distribution for each query sample\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(min(10, len(query_ids))):\n",
    "    ax = axes[i]\n",
    "    query_scores = scores[i]\n",
    "    \n",
    "    ax.hist(query_scores, bins=50, alpha=0.7, edgecolor='black')\n",
    "    ax.axvline(query_scores.mean(), color='red', linestyle='--', label=f'Mean: {query_scores.mean():.2e}')\n",
    "    ax.set_title(f'Query {i}: {Path(query_ids[i]).name}', fontsize=10)\n",
    "    ax.set_xlabel('Attribution Score')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'score_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nScore statistics:\")\n",
    "print(f\"Overall mean: {scores.mean():.4e}\")\n",
    "print(f\"Overall std: {scores.std():.4e}\")\n",
    "print(f\"Overall min: {scores.min():.4e}\")\n",
    "print(f\"Overall max: {scores.max():.4e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top/Middle/Bottom Influential Training Samples\n",
    "\n",
    "For each query sample, find:\n",
    "- **Top 5**: highest positive attribution (most influential)\n",
    "- **Middle 5**: near-zero attribution (neutral)\n",
    "- **Bottom 5**: most negative attribution (least influential / counterfactual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_audio_samples(query_idx, n_samples=5):\n",
    "    \"\"\"\n",
    "    Display query sample and its top/middle/bottom attributed training samples.\n",
    "    \"\"\"\n",
    "    query_path = query_ids[query_idx]\n",
    "    query_scores = scores[query_idx]\n",
    "    \n",
    "    # Get indices\n",
    "    sorted_indices = np.argsort(query_scores)\n",
    "    top_indices = sorted_indices[-n_samples:][::-1]  # Highest scores\n",
    "    bottom_indices = sorted_indices[:n_samples]       # Lowest scores\n",
    "    \n",
    "    # Middle: find indices closest to median\n",
    "    median_score = np.median(query_scores)\n",
    "    middle_indices = np.argsort(np.abs(query_scores - median_score))[:n_samples]\n",
    "    \n",
    "    # Display\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"QUERY SAMPLE {query_idx}: {Path(query_path).name}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if Path(query_path).exists():\n",
    "        display(HTML(f\"<h3>Query Sample</h3>\"))\n",
    "        display(Audio(query_path, rate=44100))\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Query file not found: {query_path}\")\n",
    "    \n",
    "    # Top influential\n",
    "    display(HTML(f\"<h3>üî• Top {n_samples} Most Influential Training Samples</h3>\"))\n",
    "    for rank, idx in enumerate(top_indices, 1):\n",
    "        train_path = train_ids[idx]\n",
    "        score = query_scores[idx]\n",
    "        print(f\"\\n#{rank} | Score: {score:.4e} | {Path(train_path).name}\")\n",
    "        if Path(train_path).exists():\n",
    "            display(Audio(train_path, rate=44100))\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è File not found: {train_path}\")\n",
    "    \n",
    "    # Middle neutral\n",
    "    display(HTML(f\"<h3>‚öñÔ∏è {n_samples} Neutral Training Samples (near median)</h3>\"))\n",
    "    for rank, idx in enumerate(middle_indices, 1):\n",
    "        train_path = train_ids[idx]\n",
    "        score = query_scores[idx]\n",
    "        print(f\"\\n#{rank} | Score: {score:.4e} | {Path(train_path).name}\")\n",
    "        if Path(train_path).exists():\n",
    "            display(Audio(train_path, rate=44100))\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è File not found: {train_path}\")\n",
    "    \n",
    "    # Bottom\n",
    "    display(HTML(f\"<h3>‚ùÑÔ∏è Bottom {n_samples} Least Influential Training Samples</h3>\"))\n",
    "    for rank, idx in enumerate(bottom_indices, 1):\n",
    "        train_path = train_ids[idx]\n",
    "        score = query_scores[idx]\n",
    "        print(f\"\\n#{rank} | Score: {score:.4e} | {Path(train_path).name}\")\n",
    "        if Path(train_path).exists():\n",
    "            display(Audio(train_path, rate=44100))\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è File not found: {train_path}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: Query Sample 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_audio_samples(query_idx=0, n_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: Query Sample 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_audio_samples(query_idx=1, n_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: Query Sample 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_audio_samples(query_idx=2, n_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze All Query Samples at Once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to analyze all query samples\n",
    "# for i in range(len(query_ids)):\n",
    "#     display_audio_samples(query_idx=i, n_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Top Attributions to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create summary table\n",
    "results = []\n",
    "for i in range(len(query_ids)):\n",
    "    query_scores = scores[i]\n",
    "    top_idx = np.argmax(query_scores)\n",
    "    bottom_idx = np.argmin(query_scores)\n",
    "    \n",
    "    results.append({\n",
    "        'query_idx': i,\n",
    "        'query_file': Path(query_ids[i]).name,\n",
    "        'top_train_file': Path(train_ids[top_idx]).name,\n",
    "        'top_score': query_scores[top_idx],\n",
    "        'bottom_train_file': Path(train_ids[bottom_idx]).name,\n",
    "        'bottom_score': query_scores[bottom_idx],\n",
    "        'mean_score': query_scores.mean(),\n",
    "        'std_score': query_scores.std(),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(output_dir / 'attribution_summary.csv', index=False)\n",
    "print(\"Saved attribution summary to:\", output_dir / 'attribution_summary.csv')\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dattri",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
